<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="backup-restore-over">
  <title>Backup and Restore Overview</title>
  <body>
    <p>Greenplum Database supports parallel and non-parallel methods for backing up and restoring
      databases. Parallel operations scale regardless of the number of segments in your system,
      because segment hosts each write their data to local disk storage simultaneously. With
      non-parallel backup and restore operations, the data must be sent over the network from the
      segments to the master, which writes all of the data to its storage. In addition to
      restricting I/O to one host, non-parallel backup requires that the master have sufficient
      local disk storage to store the entire database. </p>
    <section>
      <title>Parallel Backup with gpcrondump and gpdbrestore</title>
      <p>The Greenplum Database parallel dump utility <codeph>gpcrondump</codeph> backs up the
        Greenplum master instance and each active segment instance at the same time. </p>
      <p>By default, <codeph>gpcrondump</codeph> creates dump files in the <codeph>db_dumps</codeph>
        subdirectory of each segment instance. On the master, <codeph>gpcrondump</codeph> creates
        several dump files, containing database information such as DDL statements, the system
        catalog tables, and metadata files. On each segment, <codeph>gpcrondump</codeph> creates one
        dump file, which contains commands to recreate the data on that segment. Each file created
        for a backup begins with a 14-digit timestamp key that identifies the backup set the file
        belongs to. </p>
      <fig id="kk155499">
        <title>Parallel Backups in Greenplum Database</title>
        <image href="../graphics/gp_dump.jpg" placement="break" width="402px" height="226px"
          id="image_bh4_jhx_yq"/>
      </fig>
      <p>The <codeph>gpdbrestore</codeph> parallel restore utility takes the timestamp key generated
        by <codeph>gpcrondump</codeph>, validates the backup set, and restores the database objects
        and data into a distributed database. Parallel restore operations require a complete backup
        set created by <codeph>gpcrondump</codeph>, a full backup, and any required incremental
        backups. As the following figure illustrates, all segments restore data from local backup
        files simultaneously.</p>
      <fig id="kk157614">
        <title>Parallel Restores in Greenplum Database</title>
        <image href="../graphics/gp_restore.jpg" placement="break" width="437px" height="241px"
          id="image_zxw_4hx_yq"/>
      </fig>
      <p id="kk156487">The <codeph>gpdbrestore</codeph> utility provides flexibility and
        verification options for use with the automated backup files produced by
          <codeph>gpcrondump</codeph> or with backup files moved from the Greenplum cluster to an
        alternate location. See <xref href="restore-parallel.xml" type="topic" format="dita"/>.
          <codeph>gpdbrestore</codeph> can also be used to copy files to the alternate location.</p>
    </section>
    <section>
      <title id="kk155276">Non-Parallel Backup with pg_dump</title>
      <p>The PostgreSQL <codeph>pg_dump</codeph> and <codeph>pg_dumpall</codeph> non-parallel backup
        utilities can be used to create a single dump file on the master host that contains all data
        from all active segments. </p>
      <p>The PostgreSQL non-parallel utilities should be used only for special cases. They are much
        slower than using the Greenplum backup utilities since all of the data must pass through the
        master. Additionally, it is often the case that the master host has insufficient disk space
        to save a backup of an entire distributed Greenplum database. </p>
      <p>The <codeph>pg_restore</codeph> utility requires compressed dump files created by
          <codeph>pg_dump</codeph> or <codeph>pg_dumpall</codeph>. Before starting the restore, you
        should modify the <codeph>CREATE TABLE</codeph> statements in the dump files to include the
        Greenplum <codeph>DISTRIBUTED</codeph> clause. If you do not include the
          <codeph>DISTRIBUTED</codeph> clause, Greenplum Database assigns default values, which may
        not be optimal. For details, see <codeph>CREATE TABLE</codeph> in the <i>Greenplum Database
          Reference Guide</i>.</p>
      <p>To perform a non-parallel restore using parallel backup files, you can copy the backup
        files from each segment host to the master host, and then load them through the master. See
          <xref href="restore-diff-system.xml" type="topic" format="dita"/>.</p>
      <fig id="kk156418">
        <title>Non-parallel Restore Using Parallel Backup Files</title>
        <image href="../graphics/nonpar_restore.jpg" placement="break" width="390px"
          height="231px" id="image_dyn_qhx_yq"/>
      </fig>
      <p>Another non-parallel method for backing up Greenplum Database data is to use the
          <codeph>COPY TO</codeph> SQL command to copy all or a portion of a table out of the
        database to a delimited text file on the master host. </p>
    </section>
  </body>
</topic>
